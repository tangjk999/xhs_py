import json
import pandas as pd
import os
import sys
from datetime import datetime
from typing import List, Dict, Any

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import Config

class DeepSeekAnalyzer:
    def __init__(self):
        self.config = Config()
        self.api_key = self.config.DEEPSEEK_API_KEY
        self.api_base = self.config.DEEPSEEK_API_BASE
        
        if not self.api_key:
            print("è­¦å‘Š: æœªè®¾ç½®DEEPSEEK_API_KEYï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿåˆ†æ")
            self.use_mock = True
        else:
            self.use_mock = False
    
    def update_api_key(self, api_key: str):
        """æ›´æ–°APIå¯†é’¥"""
        self.api_key = api_key
        if api_key:
            self.use_mock = False
            print("APIå¯†é’¥å·²æ›´æ–°ï¼Œå°†ä½¿ç”¨çœŸå®API")
        else:
            self.use_mock = True
            print("APIå¯†é’¥å·²æ¸…ç©ºï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿåˆ†æ")
            
    def load_data(self, csv_file_path: str) -> pd.DataFrame:
        """åŠ è½½CSVæ•°æ®æ–‡ä»¶"""
        try:
            df = pd.read_csv(csv_file_path, encoding='utf-8-sig')
            print(f"æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {csv_file_path}")
            print(f"æ•°æ®è¡Œæ•°: {len(df)}")
            return df
        except Exception as e:
            print(f"åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def analyze_trends(self, data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æçƒ­é—¨è¶‹åŠ¿"""
        if data.empty:
            return {"error": "æ•°æ®ä¸ºç©º"}
            
        analysis = {
            "total_notes": len(data),
            "top_authors": [],
            "popular_topics": [],
            "engagement_analysis": {},
            "time_analysis": {},
            "recommendations": []
        }
        
        # åˆ†æçƒ­é—¨ä½œè€…
        if 'author' in data.columns:
            author_counts = data['author'].value_counts().head(5)
            analysis["top_authors"] = [
                {"author": author, "count": count} 
                for author, count in author_counts.items()
            ]
        
        # åˆ†æç‚¹èµæ•°
        if 'likes' in data.columns:
            try:
                # æ¸…ç†ç‚¹èµæ•°æ®
                likes_data = data['likes'].astype(str).str.extract('(\d+)')[0].astype(float)
                analysis["engagement_analysis"] = {
                    "avg_likes": likes_data.mean(),
                    "max_likes": likes_data.max(),
                    "min_likes": likes_data.min(),
                    "total_likes": likes_data.sum()
                }
            except:
                analysis["engagement_analysis"] = {"error": "æ— æ³•è§£æç‚¹èµæ•°æ®"}
        
        # åˆ†ææ ‡é¢˜å…³é”®è¯
        if 'title' in data.columns:
            keywords = self._extract_keywords(data['title'].tolist())
            analysis["popular_topics"] = keywords[:10]
        
        # ç”Ÿæˆå»ºè®®
        analysis["recommendations"] = self._generate_recommendations(data)
        
        return analysis
    
    def _extract_keywords(self, titles: List[str]) -> List[Dict[str, Any]]:
        """æå–æ ‡é¢˜ä¸­çš„å…³é”®è¯"""
        import re
        from collections import Counter
        
        # åˆå¹¶æ‰€æœ‰æ ‡é¢˜
        all_text = ' '.join(titles)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡å’Œè‹±æ–‡
        cleaned_text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z\s]', '', all_text)
        
        # åˆ†è¯ï¼ˆç®€å•æŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
        words = cleaned_text.split()
        
        # è¿‡æ»¤çŸ­è¯
        words = [word for word in words if len(word) > 1]
        
        # ç»Ÿè®¡è¯é¢‘
        word_counts = Counter(words)
        
        return [
            {"keyword": word, "frequency": count} 
            for word, count in word_counts.most_common(20)
        ]
    
    def _generate_recommendations(self, data: pd.DataFrame) -> List[str]:
        """ç”Ÿæˆåˆ†æå»ºè®®"""
        recommendations = []
        
        if len(data) > 0:
            recommendations.append(f"å…±åˆ†æäº† {len(data)} æ¡ç¬”è®°")
            
            if 'likes' in data.columns:
                try:
                    likes_data = data['likes'].astype(str).str.extract('(\d+)')[0].astype(float)
                    avg_likes = likes_data.mean()
                    recommendations.append(f"å¹³å‡ç‚¹èµæ•°: {avg_likes:.1f}")
                    
                    if avg_likes > 1000:
                        recommendations.append("è¯¥ä¸»é¢˜å†…å®¹å—æ¬¢è¿ç¨‹åº¦è¾ƒé«˜")
                    elif avg_likes > 100:
                        recommendations.append("è¯¥ä¸»é¢˜å†…å®¹å—æ¬¢è¿ç¨‹åº¦ä¸­ç­‰")
                    else:
                        recommendations.append("è¯¥ä¸»é¢˜å†…å®¹å—æ¬¢è¿ç¨‹åº¦è¾ƒä½")
                except:
                    recommendations.append("æ— æ³•åˆ†æç‚¹èµæ•°æ®")
            
            if 'author' in data.columns:
                unique_authors = data['author'].nunique()
                recommendations.append(f"æ¶‰åŠ {unique_authors} ä½ä½œè€…")
                
                if unique_authors < len(data) * 0.3:
                    recommendations.append("å»ºè®®å…³æ³¨å¤´éƒ¨ä½œè€…çš„å†…å®¹ç­–ç•¥")
                else:
                    recommendations.append("å†…å®¹åˆ›ä½œè€…åˆ†å¸ƒè¾ƒä¸ºåˆ†æ•£")
        
        return recommendations
    
    def analyze_with_ai(self, data: pd.DataFrame, template: str = None) -> Dict[str, Any]:
        """ä½¿ç”¨DeepSeek AIè¿›è¡Œæ·±åº¦åˆ†æ"""
        if self.use_mock:
            return self._mock_ai_analysis(data)
        
        try:
            # å‡†å¤‡æ•°æ®æ‘˜è¦
            data_summary = self._prepare_data_summary(data)
            
            # æ„å»ºåˆ†ææç¤º
            prompt = self._build_analysis_prompt(data_summary, template)
            
            # è°ƒç”¨DeepSeek API
            response = self._call_deepseek_api(prompt)
            
            return {
                "ai_analysis": response,
                "data_summary": data_summary,
                "analysis_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
        except Exception as e:
            print(f"AIåˆ†æå¤±è´¥: {e}")
            return {"error": f"AIåˆ†æå¤±è´¥: {str(e)}"}
    
    def _prepare_data_summary(self, data: pd.DataFrame) -> Dict[str, Any]:
        """å‡†å¤‡æ•°æ®æ‘˜è¦"""
        summary = {
            "total_notes": len(data),
            "columns": list(data.columns),
            "sample_titles": [],
            "sample_authors": []
        }
        
        if 'title' in data.columns:
            summary["sample_titles"] = data['title'].head(5).tolist()
        
        if 'author' in data.columns:
            summary["sample_authors"] = data['author'].head(5).tolist()
        
        return summary
    
    def _build_analysis_prompt(self, data_summary: Dict[str, Any], template: str = None) -> str:
        """æ„å»ºåˆ†ææç¤º"""
        base_prompt = f"""
è¯·åˆ†æä»¥ä¸‹å°çº¢ä¹¦ç¬”è®°æ•°æ®ï¼Œå¹¶æä¾›æ·±å…¥çš„æ´å¯Ÿå’Œå»ºè®®ï¼š

æ•°æ®æ¦‚è§ˆï¼š
- æ€»ç¬”è®°æ•°ï¼š{data_summary['total_notes']}
- æ•°æ®å­—æ®µï¼š{', '.join(data_summary['columns'])}
- æ ·æœ¬æ ‡é¢˜ï¼š{data_summary['sample_titles']}
- æ ·æœ¬ä½œè€…ï¼š{data_summary['sample_authors']}

è¯·ä»ä»¥ä¸‹è§’åº¦è¿›è¡Œåˆ†æï¼š
1. å†…å®¹è¶‹åŠ¿åˆ†æ
2. ç”¨æˆ·åå¥½åˆ†æ
3. çƒ­é—¨è¯é¢˜è¯†åˆ«
4. å†…å®¹è´¨é‡è¯„ä¼°
5. è¥é”€ç­–ç•¥å»ºè®®
6. æœªæ¥è¶‹åŠ¿é¢„æµ‹

è¯·æä¾›ç»“æ„åŒ–çš„åˆ†ææŠ¥å‘Šï¼ŒåŒ…å«å…·ä½“çš„æ•°æ®æ´å¯Ÿå’Œ actionable çš„å»ºè®®ã€‚
"""
        
        if template:
            base_prompt += f"\n\nè¯·æŒ‰ç…§ä»¥ä¸‹æ¨¡æ¿æ ¼å¼è¾“å‡ºï¼š\n{template}"
        
        return base_prompt
    
    def _call_deepseek_api(self, prompt: str) -> str:
        """è°ƒç”¨DeepSeek API"""
        try:
            import requests
            
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "deepseek-chat",
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 2000
            }
            
            response = requests.post(
                f"{self.api_base}/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content']
            else:
                raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            raise Exception(f"è°ƒç”¨DeepSeek APIæ—¶å‡ºé”™: {e}")
    
    def _mock_ai_analysis(self, data: pd.DataFrame) -> Dict[str, Any]:
        """æ¨¡æ‹ŸAIåˆ†æï¼ˆå½“APIä¸å¯ç”¨æ—¶ï¼‰"""
        return {
            "ai_analysis": f"""
åŸºäº {len(data)} æ¡å°çº¢ä¹¦ç¬”è®°çš„AIåˆ†ææŠ¥å‘Šï¼š

ğŸ“Š å†…å®¹è¶‹åŠ¿åˆ†æï¼š
- å½“å‰ä¸»é¢˜å†…å®¹å‘ˆç°å¤šæ ·åŒ–è¶‹åŠ¿
- ç”¨æˆ·æ›´å€¾å‘äºå®ç”¨æ€§å’Œå¯æ“ä½œæ€§çš„å†…å®¹
- è§†è§‰åŒ–å†…å®¹ï¼ˆå›¾ç‰‡ã€è§†é¢‘ï¼‰æ›´å—æ¬¢è¿

ğŸ¯ ç”¨æˆ·åå¥½åˆ†æï¼š
- ç”¨æˆ·å…³æ³¨å†…å®¹çš„è´¨é‡å’Œå®ç”¨æ€§
- äº’åŠ¨æ€§å¼ºçš„å†…å®¹æ›´å®¹æ˜“è·å¾—å…³æ³¨
- ä¸ªæ€§åŒ–æ¨èå¯¹ç”¨æˆ·è¡Œä¸ºå½±å“æ˜¾è‘—

ğŸ”¥ çƒ­é—¨è¯é¢˜è¯†åˆ«ï¼š
- ç”Ÿæ´»æŠ€å·§ç±»å†…å®¹æŒç»­çƒ­é—¨
- ç¾é£Ÿã€æ—…è¡Œã€æ—¶å°šæ˜¯æ°¸æ’è¯é¢˜
- æ–°å…´è¯é¢˜éœ€è¦åŠæ—¶è·Ÿè¿›

ğŸ’¡ è¥é”€ç­–ç•¥å»ºè®®ï¼š
- æ³¨é‡å†…å®¹è´¨é‡å’ŒåŸåˆ›æ€§
- å¢åŠ ä¸ç”¨æˆ·çš„äº’åŠ¨
- åˆ©ç”¨çƒ­é—¨è¯é¢˜æé«˜æ›å…‰åº¦
- å»ºç«‹ä¸ªäººå“ç‰Œå½¢è±¡

ğŸ”® æœªæ¥è¶‹åŠ¿é¢„æµ‹ï¼š
- çŸ­è§†é¢‘å†…å®¹å°†ç»§ç»­å¢é•¿
- ä¸ªæ€§åŒ–æ¨èå°†æ›´åŠ ç²¾å‡†
- ç¤¾åŒºäº’åŠ¨åŠŸèƒ½å°†æ›´åŠ é‡è¦
            """,
            "data_summary": self._prepare_data_summary(data),
            "analysis_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    
    def save_analysis(self, analysis: Dict[str, Any], filename: str = None) -> str:
        """ä¿å­˜åˆ†æç»“æœ"""
        if not filename:
            filename = f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs(self.config.DATA_DIR, exist_ok=True)
        filepath = os.path.join(self.config.DATA_DIR, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath

if __name__ == "__main__":
    # æµ‹è¯•åˆ†æå™¨
    analyzer = DeepSeekAnalyzer()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = pd.DataFrame({
        'title': ['ç¾é£Ÿæ¢åº—åˆ†äº«', 'æ—…è¡Œæ”»ç•¥', 'ç©¿æ­æŠ€å·§', 'æŠ¤è‚¤å¿ƒå¾—', 'å¥èº«æ•™ç¨‹'],
        'author': ['ç¾é£Ÿè¾¾äºº', 'æ—…è¡Œåšä¸»', 'æ—¶å°šåšä¸»', 'ç¾å¦†åšä¸»', 'å¥èº«æ•™ç»ƒ'],
        'likes': ['1.2k', '856', '2.1k', '1.5k', '3.2k'],
        'publish_time': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05']
    })
    
    # è¿›è¡Œè¶‹åŠ¿åˆ†æ
    trends = analyzer.analyze_trends(test_data)
    print("è¶‹åŠ¿åˆ†æç»“æœ:", json.dumps(trends, ensure_ascii=False, indent=2))
    
    # è¿›è¡ŒAIåˆ†æ
    ai_result = analyzer.analyze_with_ai(test_data)
    print("AIåˆ†æç»“æœ:", ai_result['ai_analysis']) 