import json
import pandas as pd
import os
import sys
from datetime import datetime
from typing import List, Dict, Any

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import Config

class DeepSeekAnalyzer:
    def __init__(self):
        self.config = Config()
        self.api_key = self.config.DEEPSEEK_API_KEY
        self.api_base = self.config.DEEPSEEK_API_BASE
        
        if not self.api_key:
            print("è­¦å‘Š: æœªè®¾ç½®DEEPSEEK_API_KEYï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿåˆ†æ")
            self.use_mock = True
        else:
            self.use_mock = False
            
    def update_api_key(self, api_key: str):
        """æ›´æ–°APIå¯†é’¥"""
        self.api_key = api_key
        if api_key:
            self.use_mock = False
            print("APIå¯†é’¥å·²æ›´æ–°ï¼Œå°†ä½¿ç”¨çœŸå®API")
        else:
            self.use_mock = True
            print("APIå¯†é’¥å·²æ¸…ç©ºï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿåˆ†æ")
            
    def load_data(self, csv_file_path: str) -> pd.DataFrame:
        """åŠ è½½CSVæ•°æ®æ–‡ä»¶"""
        try:
            df = pd.read_csv(csv_file_path, encoding='utf-8-sig')
            print(f"æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {csv_file_path}")
            print(f"æ•°æ®è¡Œæ•°: {len(df)}")
            return df
        except Exception as e:
            print(f"åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def analyze_trends(self, data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æçƒ­é—¨è¶‹åŠ¿"""
        if data.empty:
            return {"error": "æ•°æ®ä¸ºç©º"}
            
        analysis = {
            "total_notes": len(data),
            "top_authors": [],
            "popular_topics": [],
            "engagement_analysis": {},
            "time_analysis": {},
            "recommendations": []
        }
        
        # åˆ†æçƒ­é—¨ä½œè€…
        if 'author' in data.columns:
            author_counts = data['author'].value_counts().head(5)
            analysis["top_authors"] = [
                {"author": author, "count": count} 
                for author, count in author_counts.items()
            ]
        
        # åˆ†æç‚¹èµæ•°
        if 'likes' in data.columns:
            try:
                # æ¸…ç†ç‚¹èµæ•°æ®
                likes_data = data['likes'].astype(str).str.extract('(\d+)')[0].astype(float)
                analysis["engagement_analysis"] = {
                    "avg_likes": likes_data.mean(),
                    "max_likes": likes_data.max(),
                    "min_likes": likes_data.min(),
                    "total_likes": likes_data.sum()
                }
            except:
                analysis["engagement_analysis"] = {"error": "æ— æ³•è§£æç‚¹èµæ•°æ®"}
        
        # åˆ†ææ ‡é¢˜å…³é”®è¯
        if 'title' in data.columns:
            keywords = self._extract_keywords(data['title'].tolist())
            analysis["popular_topics"] = keywords[:10]
        
        # ç”Ÿæˆå»ºè®®
        analysis["recommendations"] = self._generate_recommendations(data)
        
        return analysis
    
    def _extract_keywords(self, titles: List[str]) -> List[Dict[str, Any]]:
        """æå–æ ‡é¢˜ä¸­çš„å…³é”®è¯"""
        import re
        from collections import Counter
        
        # åˆå¹¶æ‰€æœ‰æ ‡é¢˜
        all_text = ' '.join(titles)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡å’Œè‹±æ–‡
        cleaned_text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z\s]', '', all_text)
        
        # åˆ†è¯ï¼ˆç®€å•æŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
        words = cleaned_text.split()
        
        # è¿‡æ»¤çŸ­è¯
        words = [word for word in words if len(word) > 1]
        
        # ç»Ÿè®¡è¯é¢‘
        word_counts = Counter(words)
        
        return [
            {"keyword": word, "frequency": count} 
            for word, count in word_counts.most_common(20)
        ]
    
    def _generate_recommendations(self, data: pd.DataFrame) -> List[str]:
        """ç”Ÿæˆåˆ†æå»ºè®®"""
        recommendations = []
        
        if len(data) > 0:
            recommendations.append(f"å…±åˆ†æäº† {len(data)} æ¡ç¬”è®°")
            
            if 'likes' in data.columns:
                try:
                    likes_data = data['likes'].astype(str).str.extract('(\d+)')[0].astype(float)
                    avg_likes = likes_data.mean()
                    recommendations.append(f"å¹³å‡ç‚¹èµæ•°: {avg_likes:.1f}")
                    
                    if avg_likes > 1000:
                        recommendations.append("è¯¥ä¸»é¢˜å†…å®¹å—æ¬¢è¿ç¨‹åº¦è¾ƒé«˜")
                    elif avg_likes > 100:
                        recommendations.append("è¯¥ä¸»é¢˜å†…å®¹å—æ¬¢è¿ç¨‹åº¦ä¸­ç­‰")
                    else:
                        recommendations.append("è¯¥ä¸»é¢˜å†…å®¹å—æ¬¢è¿ç¨‹åº¦è¾ƒä½")
                except:
                    recommendations.append("æ— æ³•åˆ†æç‚¹èµæ•°æ®")
            
            if 'author' in data.columns:
                unique_authors = data['author'].nunique()
                recommendations.append(f"æ¶‰åŠ {unique_authors} ä½ä½œè€…")
                
                if unique_authors < len(data) * 0.3:
                    recommendations.append("å»ºè®®å…³æ³¨å¤´éƒ¨ä½œè€…çš„å†…å®¹ç­–ç•¥")
                else:
                    recommendations.append("å†…å®¹åˆ›ä½œè€…åˆ†å¸ƒè¾ƒä¸ºåˆ†æ•£")
        
        return recommendations
    
    def analyze_with_ai(self, data: pd.DataFrame, template: str = None) -> Dict[str, Any]:
        """ä½¿ç”¨DeepSeek AIè¿›è¡Œæ·±åº¦åˆ†æ"""
        if self.use_mock:
            return self._mock_ai_analysis(data)
        
        try:
            # å‡†å¤‡æ•°æ®æ‘˜è¦
            data_summary = self._prepare_data_summary(data)
            
            # æ„å»ºåˆ†ææç¤º
            prompt = self._build_analysis_prompt(data_summary, template)
            
            # è°ƒç”¨DeepSeek API
            response = self._call_deepseek_api(prompt)
            
            return {
                "ai_analysis": response,
                "data_summary": data_summary,
                "analysis_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
        except Exception as e:
            print(f"AIåˆ†æå¤±è´¥: {e}")
            return {"error": f"AIåˆ†æå¤±è´¥: {str(e)}"}
    
    def _prepare_data_summary(self, data: pd.DataFrame) -> Dict[str, Any]:
        """å‡†å¤‡æ•°æ®æ‘˜è¦"""
        summary = {
            "total_notes": len(data),
            "columns": list(data.columns),
            "sample_titles": [],
            "sample_authors": []
        }
        
        if 'title' in data.columns:
            summary["sample_titles"] = data['title'].head(5).tolist()
        
        if 'author' in data.columns:
            summary["sample_authors"] = data['author'].head(5).tolist()
        
        return summary
    
    def _build_analysis_prompt(self, data_summary: Dict[str, Any], template: str = None) -> str:
        """æ„å»ºåˆ†ææç¤º"""
        base_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°çº¢ä¹¦å†…å®¹åˆ†æå¸ˆï¼Œè¯·å¯¹ä»¥ä¸‹å°çº¢ä¹¦ç¬”è®°æ•°æ®è¿›è¡Œæ·±åº¦åˆ†æï¼Œå¹¶æä¾›æœ‰ä»·å€¼çš„æ´å¯Ÿå’Œå»ºè®®ã€‚

## æ•°æ®æ¦‚è§ˆ
- æ€»ç¬”è®°æ•°ï¼š{data_summary['total_notes']}
- æ•°æ®å­—æ®µï¼š{', '.join(data_summary['columns'])}
- æ ·æœ¬æ ‡é¢˜ï¼š{data_summary['sample_titles']}
- æ ·æœ¬ä½œè€…ï¼š{data_summary['sample_authors']}

## åˆ†æè¦æ±‚
è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œä¸“ä¸šåˆ†æï¼š

### 1. å†…å®¹è¶‹åŠ¿åˆ†æ
- çƒ­é—¨è¯é¢˜å’Œå…³é”®è¯è¯†åˆ«
- å†…å®¹ç±»å‹åˆ†å¸ƒ
- æ ‡é¢˜ç‰¹å¾åˆ†æ
- å†…å®¹é£æ ¼æ€»ç»“

### 2. ç”¨æˆ·è¡Œä¸ºåˆ†æ
- ç‚¹èµæ•°åˆ†å¸ƒå’Œè§„å¾‹
- çƒ­é—¨ä½œè€…ç‰¹å¾
- ç”¨æˆ·åå¥½åˆ†æ
- äº’åŠ¨æ¨¡å¼æ€»ç»“

### 3. å¸‚åœºæ´å¯Ÿ
- è¡Œä¸šè¶‹åŠ¿åˆ¤æ–­
- ç”¨æˆ·éœ€æ±‚åˆ†æ
- å†…å®¹æœºä¼šè¯†åˆ«
- ç«äº‰æ€åŠ¿è¯„ä¼°

### 4. ç­–ç•¥å»ºè®®
- å†…å®¹åˆ›ä½œå»ºè®®
- è¿è¥ç­–ç•¥æ¨è
- ç”¨æˆ·å¢é•¿å»ºè®®
- å˜ç°æœºä¼šåˆ†æ

### 5. é£é™©æç¤º
- æ½œåœ¨é£é™©è¯†åˆ«
- åˆè§„å»ºè®®
- ç«äº‰é£é™©åˆ†æ

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œåˆ†æè¦å…·ä½“ã€å®ç”¨ã€æœ‰æ•°æ®æ”¯æ’‘ã€‚å¦‚æœæ•°æ®ä¸è¶³ï¼Œè¯·è¯´æ˜å¹¶ç»™å‡ºåŸºäºç»éªŒçš„å»ºè®®ã€‚
"""
        
        if template:
            # å¦‚æœæœ‰è‡ªå®šä¹‰æ¨¡æ¿ï¼Œä½¿ç”¨æ¨¡æ¿
            try:
                with open(template, 'r', encoding='utf-8') as f:
                    template_content = f.read()
                return template_content.format(**data_summary)
            except Exception as e:
                print(f"æ¨¡æ¿åŠ è½½å¤±è´¥: {e}")
        
        return base_prompt
    
    def _call_deepseek_api(self, prompt: str) -> str:
        """è°ƒç”¨DeepSeek API"""
        try:
            import requests
            
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            
            data = {
                'model': 'deepseek-chat',
                'messages': [
                    {
                        'role': 'system',
                        'content': 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°çº¢ä¹¦å†…å®¹åˆ†æå¸ˆï¼Œæ“…é•¿æ•°æ®åˆ†æå’Œå¸‚åœºæ´å¯Ÿã€‚'
                    },
                    {
                        'role': 'user',
                        'content': prompt
                    }
                ],
                'max_tokens': self.config.MAX_TOKENS,
                'temperature': self.config.TEMPERATURE,
                'stream': False
            }
            
            response = requests.post(
                f"{self.api_base}/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content']
            else:
                print(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                return f"APIè°ƒç”¨å¤±è´¥: {response.status_code}"
                
        except Exception as e:
            print(f"è°ƒç”¨DeepSeek APIæ—¶å‡ºé”™: {e}")
            return f"APIè°ƒç”¨å‡ºé”™: {str(e)}"
    
    def _mock_ai_analysis(self, data: pd.DataFrame) -> Dict[str, Any]:
        """æ¨¡æ‹ŸAIåˆ†æï¼ˆå½“APIä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰"""
        print("ğŸ­ ä½¿ç”¨æ¨¡æ‹ŸAIåˆ†æ...")
        
        analysis = {
            "content_trends": {
                "hot_topics": ["ç¾é£Ÿåˆ†äº«", "ç”Ÿæ´»è®°å½•", "è´­ç‰©æ¨è", "æ—…è¡Œæ”»ç•¥", "ç¾å¦†æ•™ç¨‹"],
                "content_types": ["å›¾æ–‡ç¬”è®°", "è§†é¢‘å†…å®¹", "åˆé›†æ¨è"],
                "title_patterns": ["æ•°å­—+å…³é”®è¯", "æƒ…æ„ŸåŒ–è¡¨è¾¾", "å®ç”¨æ€§å¼º"],
                "style_summary": "å†…å®¹é£æ ¼åå‘å®ç”¨æ€§å’Œç”Ÿæ´»åŒ–"
            },
            "user_behavior": {
                "engagement_patterns": "ç”¨æˆ·æ›´å€¾å‘äºç‚¹èµå®ç”¨æ€§å¼ºçš„å†…å®¹",
                "author_insights": "å¤´éƒ¨ä½œè€…å†…å®¹è´¨é‡è¾ƒé«˜ï¼Œäº’åŠ¨æ€§å¼º",
                "user_preferences": "ç¾é£Ÿã€ç”Ÿæ´»ã€ç¾å¦†ç±»å†…å®¹å—æ¬¢è¿",
                "interaction_summary": "è¯„è®ºå’Œæ”¶è—æ˜¯é‡è¦çš„äº’åŠ¨æŒ‡æ ‡"
            },
            "market_insights": {
                "industry_trends": "å†…å®¹åˆ›ä½œå‘ä¸“ä¸šåŒ–ã€å‚ç›´åŒ–å‘å±•",
                "user_needs": "ç”¨æˆ·éœ€è¦æ›´å¤šå®ç”¨ã€çœŸå®çš„å†…å®¹",
                "opportunities": "ç»†åˆ†é¢†åŸŸä»æœ‰è¾ƒå¤§å‘å±•ç©ºé—´",
                "competition": "å†…å®¹åŒè´¨åŒ–ä¸¥é‡ï¼Œéœ€è¦å·®å¼‚åŒ–ç«äº‰"
            },
            "strategic_recommendations": {
                "content_strategy": [
                    "æ³¨é‡å†…å®¹è´¨é‡å’ŒåŸåˆ›æ€§",
                    "å»ºç«‹ä¸ªäººå“ç‰Œå’Œç‰¹è‰²",
                    "ä¿æŒæ›´æ–°é¢‘ç‡å’Œäº’åŠ¨æ€§",
                    "å…³æ³¨ç”¨æˆ·åé¦ˆå’Œéœ€æ±‚"
                ],
                "growth_tips": [
                    "é€‰æ‹©åˆé€‚çš„ç»†åˆ†é¢†åŸŸæ·±è€•",
                    "ä¸å…¶ä»–åˆ›ä½œè€…åˆä½œäº’åŠ¨",
                    "åˆ©ç”¨çƒ­ç‚¹è¯é¢˜å¢åŠ æ›å…‰",
                    "å»ºç«‹ç²‰ä¸ç¤¾ç¾¤"
                ],
                "monetization": [
                    "é€šè¿‡ä¼˜è´¨å†…å®¹å¸å¼•å“ç‰Œåˆä½œ",
                    "å¼€å‘è‡ªæœ‰äº§å“æˆ–æœåŠ¡",
                    "åˆ©ç”¨å¹³å°å˜ç°åŠŸèƒ½",
                    "å»ºç«‹å¤šå…ƒåŒ–æ”¶å…¥æ¥æº"
                ]
            },
            "risk_warnings": {
                "compliance_risks": "æ³¨æ„å†…å®¹åˆè§„æ€§ï¼Œé¿å…è¿è§„",
                "competition_risks": "å¸‚åœºç«äº‰æ¿€çƒˆï¼Œéœ€è¦æŒç»­åˆ›æ–°",
                "platform_risks": "å¹³å°æ”¿ç­–å˜åŒ–å¯èƒ½å½±å“è¿è¥"
            }
        }
        
        return analysis
    
    def save_analysis(self, analysis: Dict[str, Any], filename: str = None) -> str:
        """ä¿å­˜åˆ†æç»“æœ"""
        if not filename:
            filename = f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs(self.config.DATA_DIR, exist_ok=True)
        filepath = os.path.join(self.config.DATA_DIR, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath

    def generate_comprehensive_report(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
        
        # åŸºç¡€è¶‹åŠ¿åˆ†æ
        trends = self.analyze_trends(data)
        
        # AIæ·±åº¦åˆ†æ
        ai_result = self.analyze_with_ai(data)
        
        # æ•°æ®ç»Ÿè®¡
        stats = self._calculate_statistics(data)
        
        # å¯è§†åŒ–æ•°æ®
        chart_data = self._prepare_chart_data(data)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "summary": {
                "total_notes": len(data),
                "analysis_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                "data_source": "å°çº¢ä¹¦çˆ¬è™«",
                "analysis_version": "1.0.0"
            },
            "trends": trends,
            "ai_analysis": ai_result,
            "statistics": stats,
            "charts": chart_data,
            "recommendations": self._generate_actionable_recommendations(data, trends, ai_result)
        }
        
        return report

    def _calculate_statistics(self, data: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—è¯¦ç»†ç»Ÿè®¡æ•°æ®"""
        stats = {
            "basic_stats": {},
            "engagement_stats": {},
            "author_stats": {},
            "content_stats": {}
        }
        
        # åŸºç¡€ç»Ÿè®¡
        stats["basic_stats"] = {
            "total_notes": len(data),
            "unique_authors": data['author'].nunique() if 'author' in data.columns else 0,
            "date_range": self._get_date_range(data) if 'publish_time' in data.columns else "æœªçŸ¥"
        }
        
        # äº’åŠ¨ç»Ÿè®¡
        if 'likes' in data.columns:
            try:
                likes_data = data['likes'].astype(str).str.extract('(\d+)')[0].astype(float)
                stats["engagement_stats"] = {
                    "avg_likes": likes_data.mean(),
                    "median_likes": likes_data.median(),
                    "max_likes": likes_data.max(),
                    "min_likes": likes_data.min(),
                    "total_likes": likes_data.sum(),
                    "likes_distribution": {
                        "high": len(likes_data[likes_data > likes_data.quantile(0.8)]),
                        "medium": len(likes_data[(likes_data > likes_data.quantile(0.2)) & (likes_data <= likes_data.quantile(0.8))]),
                        "low": len(likes_data[likes_data <= likes_data.quantile(0.2)])
                    }
                }
            except:
                stats["engagement_stats"] = {"error": "æ— æ³•è§£æç‚¹èµæ•°æ®"}
        
        # ä½œè€…ç»Ÿè®¡
        if 'author' in data.columns:
            author_counts = data['author'].value_counts()
            stats["author_stats"] = {
                "top_authors": author_counts.head(10).to_dict(),
                "author_distribution": {
                    "single_post": len(author_counts[author_counts == 1]),
                    "multiple_posts": len(author_counts[author_counts > 1])
                }
            }
        
        # å†…å®¹ç»Ÿè®¡
        if 'title' in data.columns:
            title_lengths = data['title'].str.len()
            stats["content_stats"] = {
                "avg_title_length": title_lengths.mean(),
                "title_length_distribution": {
                    "short": len(title_lengths[title_lengths <= 10]),
                    "medium": len(title_lengths[(title_lengths > 10) & (title_lengths <= 30)]),
                    "long": len(title_lengths[title_lengths > 30])
                }
            }
        
        return stats

    def _get_date_range(self, data: pd.DataFrame) -> str:
        """è·å–æ•°æ®çš„æ—¶é—´èŒƒå›´"""
        try:
            # å°è¯•è§£æå‘å¸ƒæ—¶é—´
            dates = pd.to_datetime(data['publish_time'], errors='coerce')
            valid_dates = dates.dropna()
            if len(valid_dates) > 0:
                start_date = valid_dates.min().strftime('%Y-%m-%d')
                end_date = valid_dates.max().strftime('%Y-%m-%d')
                return f"{start_date} è‡³ {end_date}"
        except:
            pass
        return "æ—¶é—´èŒƒå›´æœªçŸ¥"

    def _prepare_chart_data(self, data: pd.DataFrame) -> Dict[str, Any]:
        """å‡†å¤‡å›¾è¡¨æ•°æ®"""
        chart_data = {}
        
        # ä½œè€…åˆ†å¸ƒå›¾
        if 'author' in data.columns:
            author_counts = data['author'].value_counts().head(10)
            chart_data["author_distribution"] = {
                "labels": author_counts.index.tolist(),
                "data": author_counts.values.tolist()
            }
        
        # ç‚¹èµæ•°åˆ†å¸ƒå›¾
        if 'likes' in data.columns:
            try:
                likes_data = data['likes'].astype(str).str.extract('(\d+)')[0].astype(float)
                # åˆ›å»ºç‚¹èµæ•°åŒºé—´
                bins = [0, 100, 500, 1000, 5000, float('inf')]
                labels = ['0-100', '101-500', '501-1000', '1001-5000', '5000+']
                likes_binned = pd.cut(likes_data, bins=bins, labels=labels, include_lowest=True)
                likes_dist = likes_binned.value_counts()
                chart_data["likes_distribution"] = {
                    "labels": likes_dist.index.tolist(),
                    "data": likes_dist.values.tolist()
                }
            except:
                pass
        
        # æ ‡é¢˜é•¿åº¦åˆ†å¸ƒ
        if 'title' in data.columns:
            title_lengths = data['title'].str.len()
            bins = [0, 10, 20, 30, 50, float('inf')]
            labels = ['0-10', '11-20', '21-30', '31-50', '50+']
            length_binned = pd.cut(title_lengths, bins=bins, labels=labels, include_lowest=True)
            length_dist = length_binned.value_counts()
            chart_data["title_length_distribution"] = {
                "labels": length_dist.index.tolist(),
                "data": length_dist.values.tolist()
            }
        
        return chart_data

    def _generate_actionable_recommendations(self, data: pd.DataFrame, trends: Dict, ai_result: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆå¯æ‰§è¡Œçš„å»ºè®®"""
        recommendations = {
            "content_strategy": [],
            "growth_tips": [],
            "monetization": [],
            "risk_management": []
        }
        
        # åŸºäºæ•°æ®çš„å»ºè®®
        if trends.get('engagement_analysis', {}).get('avg_likes', 0) > 1000:
            recommendations["content_strategy"].append("å†…å®¹è´¨é‡è¾ƒé«˜ï¼Œå»ºè®®ä¿æŒç°æœ‰åˆ›ä½œé£æ ¼")
        else:
            recommendations["content_strategy"].append("å»ºè®®æå‡å†…å®¹è´¨é‡å’Œäº’åŠ¨æ€§")
        
        if trends.get('top_authors'):
            recommendations["growth_tips"].append(f"å…³æ³¨å¤´éƒ¨ä½œè€…ï¼ˆå¦‚{trends['top_authors'][0]['author']}ï¼‰çš„åˆ›ä½œç­–ç•¥")
        
        if ai_result.get('ai_analysis'):
            # ä»AIåˆ†æä¸­æå–å»ºè®®
            ai_text = ai_result['ai_analysis']
            if isinstance(ai_text, str):
                if "å®ç”¨" in ai_text:
                    recommendations["content_strategy"].append("æ³¨é‡å†…å®¹çš„å®ç”¨æ€§")
                if "åŸåˆ›" in ai_text:
                    recommendations["content_strategy"].append("ä¿æŒå†…å®¹åŸåˆ›æ€§")
                if "äº’åŠ¨" in ai_text:
                    recommendations["growth_tips"].append("åŠ å¼ºä¸ç”¨æˆ·çš„äº’åŠ¨")
        
        # é€šç”¨å»ºè®®
        recommendations["content_strategy"].extend([
            "å®šæœŸåˆ†æçƒ­é—¨è¯é¢˜å’Œå…³é”®è¯",
            "ä¿æŒå†…å®¹æ›´æ–°é¢‘ç‡",
            "å»ºç«‹ä¸ªäººå“ç‰Œç‰¹è‰²"
        ])
        
        recommendations["growth_tips"].extend([
            "ä¸å…¶ä»–åˆ›ä½œè€…åˆä½œäº’åŠ¨",
            "åˆ©ç”¨çƒ­ç‚¹è¯é¢˜å¢åŠ æ›å…‰",
            "å»ºç«‹ç²‰ä¸ç¤¾ç¾¤"
        ])
        
        recommendations["monetization"].extend([
            "é€šè¿‡ä¼˜è´¨å†…å®¹å¸å¼•å“ç‰Œåˆä½œ",
            "å¼€å‘è‡ªæœ‰äº§å“æˆ–æœåŠ¡",
            "åˆ©ç”¨å¹³å°å˜ç°åŠŸèƒ½"
        ])
        
        recommendations["risk_management"].extend([
            "æ³¨æ„å†…å®¹åˆè§„æ€§",
            "å…³æ³¨å¹³å°æ”¿ç­–å˜åŒ–",
            "å»ºç«‹å¤šå…ƒåŒ–æ”¶å…¥æ¥æº"
        ])
        
        return recommendations

if __name__ == "__main__":
    # æµ‹è¯•åˆ†æå™¨
    analyzer = DeepSeekAnalyzer()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = pd.DataFrame({
        'title': ['ç¾é£Ÿæ¢åº—åˆ†äº«', 'æ—…è¡Œæ”»ç•¥', 'ç©¿æ­æŠ€å·§', 'æŠ¤è‚¤å¿ƒå¾—', 'å¥èº«æ•™ç¨‹'],
        'author': ['ç¾é£Ÿè¾¾äºº', 'æ—…è¡Œåšä¸»', 'æ—¶å°šåšä¸»', 'ç¾å¦†åšä¸»', 'å¥èº«æ•™ç»ƒ'],
        'likes': ['1.2k', '856', '2.1k', '1.5k', '3.2k'],
        'publish_time': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05']
    })
    
    # è¿›è¡Œè¶‹åŠ¿åˆ†æ
    trends = analyzer.analyze_trends(test_data)
    print("è¶‹åŠ¿åˆ†æç»“æœ:", json.dumps(trends, ensure_ascii=False, indent=2))
    
    # è¿›è¡ŒAIåˆ†æ
    ai_result = analyzer.analyze_with_ai(test_data)
    print("AIåˆ†æç»“æœ:", ai_result['ai_analysis']) 