#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç³»ç»Ÿæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯å°çº¢ä¹¦çƒ­é—¨åšå®¢åˆ†æç³»ç»Ÿçš„å„ä¸ªæ¨¡å—åŠŸèƒ½
"""

import os
import sys
import json
import pandas as pd
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import Config
from crawler.xhs_crawler import XHSCrawler
from ai_analyzer.deepseek_analyzer import DeepSeekAnalyzer

def test_config():
    """æµ‹è¯•é…ç½®æ¨¡å—"""
    print("ğŸ”§ æµ‹è¯•é…ç½®æ¨¡å—...")
    try:
        config = Config()
        print(f"   âœ… é…ç½®åŠ è½½æˆåŠŸ")
        print(f"   ğŸ“ æ•°æ®ç›®å½•: {config.DATA_DIR}")
        print(f"   ğŸŒ APIåŸºç¡€URL: {config.DEEPSEEK_API_BASE}")
        return True
    except Exception as e:
        print(f"   âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False

def test_crawler():
    """æµ‹è¯•çˆ¬è™«æ¨¡å—"""
    print("\nğŸ•·ï¸ æµ‹è¯•çˆ¬è™«æ¨¡å—...")
    try:
        crawler = XHSCrawler()
        print("   âœ… çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = [
            {
                'title': 'ç¾é£Ÿæ¢åº—åˆ†äº«',
                'author': 'ç¾é£Ÿè¾¾äºº',
                'likes': '1.2k',
                'link': 'https://example.com/1',
                'publish_time': '2024-01-01',
                'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            },
            {
                'title': 'æ—…è¡Œæ”»ç•¥',
                'author': 'æ—…è¡Œåšä¸»',
                'likes': '856',
                'link': 'https://example.com/2',
                'publish_time': '2024-01-02',
                'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            },
            {
                'title': 'ç©¿æ­æŠ€å·§',
                'author': 'æ—¶å°šåšä¸»',
                'likes': '2.1k',
                'link': 'https://example.com/3',
                'publish_time': '2024-01-03',
                'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
        ]
        
        # ä¿å­˜æµ‹è¯•æ•°æ®
        filepath = crawler.save_to_csv(test_data, 'test_data.csv')
        print(f"   âœ… æµ‹è¯•æ•°æ®ä¿å­˜æˆåŠŸ: {filepath}")
        
        return True, filepath
    except Exception as e:
        print(f"   âŒ çˆ¬è™«æµ‹è¯•å¤±è´¥: {e}")
        return False, None

def test_analyzer():
    """æµ‹è¯•åˆ†æå™¨æ¨¡å—"""
    print("\nğŸ¤– æµ‹è¯•åˆ†æå™¨æ¨¡å—...")
    try:
        analyzer = DeepSeekAnalyzer()
        print("   âœ… åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_df = pd.DataFrame({
            'title': ['ç¾é£Ÿæ¢åº—åˆ†äº«', 'æ—…è¡Œæ”»ç•¥', 'ç©¿æ­æŠ€å·§', 'æŠ¤è‚¤å¿ƒå¾—', 'å¥èº«æ•™ç¨‹'],
            'author': ['ç¾é£Ÿè¾¾äºº', 'æ—…è¡Œåšä¸»', 'æ—¶å°šåšä¸»', 'ç¾å¦†åšä¸»', 'å¥èº«æ•™ç»ƒ'],
            'likes': ['1.2k', '856', '2.1k', '1.5k', '3.2k'],
            'publish_time': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04', '2024-01-05']
        })
        
        # æµ‹è¯•è¶‹åŠ¿åˆ†æ
        trends = analyzer.analyze_trends(test_df)
        print("   âœ… è¶‹åŠ¿åˆ†æå®Œæˆ")
        print(f"   ğŸ“Š æ€»ç¬”è®°æ•°: {trends.get('total_notes', 0)}")
        print(f"   ğŸ‘¥ çƒ­é—¨ä½œè€…æ•°: {len(trends.get('top_authors', []))}")
        print(f"   ğŸ”¥ çƒ­é—¨è¯é¢˜æ•°: {len(trends.get('popular_topics', []))}")
        
        # æµ‹è¯•AIåˆ†æ
        ai_result = analyzer.analyze_with_ai(test_df)
        print("   âœ… AIåˆ†æå®Œæˆ")
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_result = {
            'trends': trends,
            'ai_analysis': ai_result,
            'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'test_mode': True
        }
        
        analysis_file = analyzer.save_analysis(analysis_result, 'test_analysis.json')
        print(f"   ğŸ“„ åˆ†æç»“æœä¿å­˜æˆåŠŸ: {analysis_file}")
        
        return True
    except Exception as e:
        print(f"   âŒ åˆ†æå™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_web_app():
    """æµ‹è¯•Webåº”ç”¨æ¨¡å—"""
    print("\nğŸŒ æµ‹è¯•Webåº”ç”¨æ¨¡å—...")
    try:
        from web_app.app import app
        print("   âœ… Webåº”ç”¨å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•åº”ç”¨é…ç½®
        with app.test_client() as client:
            # æµ‹è¯•å¥åº·æ£€æŸ¥
            response = client.get('/api/health')
            if response.status_code == 200:
                print("   âœ… å¥åº·æ£€æŸ¥APIæ­£å¸¸")
            else:
                print(f"   âŒ å¥åº·æ£€æŸ¥APIå¤±è´¥: {response.status_code}")
                return False
            
            # æµ‹è¯•æ–‡ä»¶åˆ—è¡¨API
            response = client.get('/api/files')
            if response.status_code == 200:
                print("   âœ… æ–‡ä»¶åˆ—è¡¨APIæ­£å¸¸")
            else:
                print(f"   âŒ æ–‡ä»¶åˆ—è¡¨APIå¤±è´¥: {response.status_code}")
                return False
        
        return True
    except Exception as e:
        print(f"   âŒ Webåº”ç”¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_data_flow():
    """æµ‹è¯•å®Œæ•´æ•°æ®æµ"""
    print("\nğŸ”„ æµ‹è¯•å®Œæ•´æ•°æ®æµ...")
    try:
        # 1. åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = [
            {
                'title': 'ç¾é£Ÿæ¢åº—åˆ†äº«',
                'author': 'ç¾é£Ÿè¾¾äºº',
                'likes': '1.2k',
                'link': 'https://example.com/1',
                'publish_time': '2024-01-01',
                'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            },
            {
                'title': 'æ—…è¡Œæ”»ç•¥',
                'author': 'æ—…è¡Œåšä¸»',
                'likes': '856',
                'link': 'https://example.com/2',
                'publish_time': '2024-01-02',
                'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
        ]
        
        # 2. ä¿å­˜æ•°æ®
        crawler = XHSCrawler()
        csv_file = crawler.save_to_csv(test_data, 'flow_test.csv')
        print("   âœ… æ­¥éª¤1: æ•°æ®ä¿å­˜å®Œæˆ")
        
        # 3. åŠ è½½æ•°æ®
        analyzer = DeepSeekAnalyzer()
        df = analyzer.load_data(csv_file)
        print(f"   âœ… æ­¥éª¤2: æ•°æ®åŠ è½½å®Œæˆ ({len(df)} æ¡è®°å½•)")
        
        # 4. åˆ†ææ•°æ®
        trends = analyzer.analyze_trends(df)
        ai_result = analyzer.analyze_with_ai(df)
        print("   âœ… æ­¥éª¤3: æ•°æ®åˆ†æå®Œæˆ")
        
        # 5. ä¿å­˜åˆ†æç»“æœ
        analysis_result = {
            'trends': trends,
            'ai_analysis': ai_result,
            'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'data_file': csv_file
        }
        
        analysis_file = analyzer.save_analysis(analysis_result, 'flow_test_analysis.json')
        print("   âœ… æ­¥éª¤4: åˆ†æç»“æœä¿å­˜å®Œæˆ")
        
        # 6. éªŒè¯ç»“æœ
        if os.path.exists(csv_file) and os.path.exists(analysis_file):
            print("   âœ… æ­¥éª¤5: æ–‡ä»¶éªŒè¯é€šè¿‡")
            return True
        else:
            print("   âŒ æ­¥éª¤5: æ–‡ä»¶éªŒè¯å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"   âŒ æ•°æ®æµæµ‹è¯•å¤±è´¥: {e}")
        return False

def cleanup_test_files():
    """æ¸…ç†æµ‹è¯•æ–‡ä»¶"""
    print("\nğŸ§¹ æ¸…ç†æµ‹è¯•æ–‡ä»¶...")
    test_files = [
        'test_data.csv',
        'test_analysis.json',
        'flow_test.csv',
        'flow_test_analysis.json'
    ]
    
    config = Config()
    for filename in test_files:
        filepath = os.path.join(config.DATA_DIR, filename)
        if os.path.exists(filepath):
            try:
                os.remove(filepath)
                print(f"   ğŸ—‘ï¸ åˆ é™¤: {filename}")
            except Exception as e:
                print(f"   âŒ åˆ é™¤å¤±è´¥ {filename}: {e}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª å°çº¢ä¹¦çƒ­é—¨åšå®¢åˆ†æç³»ç»Ÿ - åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
    config = Config()
    os.makedirs(config.DATA_DIR, exist_ok=True)
    
    # æµ‹è¯•ç»“æœç»Ÿè®¡
    test_results = []
    
    # 1. æµ‹è¯•é…ç½®æ¨¡å—
    test_results.append(("é…ç½®æ¨¡å—", test_config()))
    
    # 2. æµ‹è¯•çˆ¬è™«æ¨¡å—
    crawler_success, csv_file = test_crawler()
    test_results.append(("çˆ¬è™«æ¨¡å—", crawler_success))
    
    # 3. æµ‹è¯•åˆ†æå™¨æ¨¡å—
    test_results.append(("åˆ†æå™¨æ¨¡å—", test_analyzer()))
    
    # 4. æµ‹è¯•Webåº”ç”¨æ¨¡å—
    test_results.append(("Webåº”ç”¨æ¨¡å—", test_web_app()))
    
    # 5. æµ‹è¯•å®Œæ•´æ•°æ®æµ
    test_results.append(("å®Œæ•´æ•°æ®æµ", test_data_flow()))
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("\nğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    print("=" * 50)
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   {test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {passed}/{total} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³æ¨¡å—ã€‚")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    cleanup_test_files()
    
    print("\nâœ¨ æµ‹è¯•å®Œæˆï¼")

if __name__ == '__main__':
    main() 