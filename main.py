#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦çƒ­é—¨åšå®¢åˆ†æç³»ç»Ÿ
ä¸»ç¨‹åºå…¥å£
"""

import os
import sys
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import Config
from crawler.xhs_crawler import XHSCrawler
from ai_analyzer.deepseek_analyzer import DeepSeekAnalyzer
from web_app.app import app

def print_banner():
    """æ‰“å°é¡¹ç›®æ¨ªå¹…"""
    banner = """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                              â•‘
    â•‘    å°çº¢ä¹¦çƒ­é—¨åšå®¢åˆ†æç³»ç»Ÿ v1.0.0                              â•‘
    â•‘                                                              â•‘
    â•‘    ğŸ•·ï¸  æ™ºèƒ½çˆ¬å–  |  ğŸ¤– AIåˆ†æ  |  ğŸ“Š å¯è§†åŒ–å±•ç¤º              â•‘
    â•‘                                                              â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)

def crawl_mode(args):
    """çˆ¬å–æ¨¡å¼"""
    print("ğŸ•·ï¸ å¯åŠ¨çˆ¬å–æ¨¡å¼...")
    
    # å‚æ•°éªŒè¯
    if not args.topic:
        print("âŒ é”™è¯¯: è¯·æä¾›æœç´¢ä¸»é¢˜")
        return
    
    if args.limit <= 0 or args.limit > 100:
        print("âŒ é”™è¯¯: è·å–æ•°é‡å¿…é¡»åœ¨1-100ä¹‹é—´")
        return
    
    print(f"ğŸ“Š çˆ¬å–å‚æ•°:")
    print(f"   ä¸»é¢˜: {args.topic}")
    print(f"   æ•°é‡: {args.limit}")
    print(f"   è¾“å‡ºæ–‡ä»¶: {args.output if args.output else 'è‡ªåŠ¨ç”Ÿæˆ'}")
    
    # æ‰§è¡Œçˆ¬å–
    try:
        crawler = XHSCrawler()
        result = crawler.crawl_hot_notes(args.topic, args.limit, args.cookies)
        
        if result:
            print(f"âœ… çˆ¬å–å®Œæˆï¼æ•°æ®å·²ä¿å­˜åˆ°: {result}")
            
            if args.analyze:
                print("ğŸ¤– å¼€å§‹AIåˆ†æ...")
                analyzer = DeepSeekAnalyzer()
                
                # åŠ è½½æ•°æ®
                df = analyzer.load_data(result)
                if not df.empty:
                    # è¿›è¡Œç»¼åˆåˆ†æ
                    report = analyzer.generate_comprehensive_report(df)
                    print("ğŸ“Š ç»¼åˆåˆ†æå®Œæˆ")
                    
                    # ä¿å­˜åˆ†æç»“æœ
                    analysis_file = analyzer.save_analysis(report)
                    print(f"ğŸ“„ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {analysis_file}")
                    
                    # æ‰“å°ç®€è¦ç»“æœ
                    print("\nğŸ“Š ç®€è¦åˆ†æç»“æœ:")
                    summary = report.get('summary', {})
                    print(f"   æ€»ç¬”è®°æ•°: {summary.get('total_notes', 0)}")
                    
                    stats = report.get('statistics', {})
                    engagement = stats.get('engagement_stats', {})
                    if engagement and not engagement.get('error'):
                        print(f"   å¹³å‡ç‚¹èµ: {engagement.get('avg_likes', 0):.1f}")
                        print(f"   æœ€é«˜ç‚¹èµ: {engagement.get('max_likes', 0):.1f}")
                    
                    basic_stats = stats.get('basic_stats', {})
                    print(f"   ä½œè€…æ•°é‡: {basic_stats.get('unique_authors', 0)}")
                    
                    # æ˜¾ç¤ºçƒ­é—¨ä½œè€…
                    trends = report.get('trends', {})
                    top_authors = trends.get('top_authors', [])
                    if top_authors:
                        print(f"   çƒ­é—¨ä½œè€…: {top_authors[0]['author']} ({top_authors[0]['count']}ç¯‡)")
                    
                    # æ˜¾ç¤ºçƒ­é—¨è¯é¢˜
                    popular_topics = trends.get('popular_topics', [])
                    if popular_topics:
                        print(f"   çƒ­é—¨è¯é¢˜: {popular_topics[0]['keyword']} ({popular_topics[0]['frequency']}æ¬¡)")
                else:
                    print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        else:
            print("âŒ çˆ¬å–å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

def analyze_mode(args):
    """åˆ†ææ¨¡å¼"""
    print("ğŸ¤– å¯åŠ¨åˆ†ææ¨¡å¼...")
    
    # å‚æ•°éªŒè¯
    if not os.path.exists(args.file):
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
        return
    
    print(f"ğŸ“Š åˆ†æå‚æ•°:")
    print(f"   æ•°æ®æ–‡ä»¶: {args.file}")
    print(f"   åˆ†æç±»å‹: {args.type}")
    print(f"   è¾“å‡ºæ–‡ä»¶: {args.output if args.output else 'è‡ªåŠ¨ç”Ÿæˆ'}")
    
    try:
        analyzer = DeepSeekAnalyzer()
        
        # åŠ è½½æ•°æ®
        df = analyzer.load_data(args.file)
        if df.empty:
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
            return
        
        print(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(df)} æ¡æ•°æ®")
        
        # æ ¹æ®åˆ†æç±»å‹æ‰§è¡Œä¸åŒçš„åˆ†æ
        if args.type == 'comprehensive':
            print("ğŸ“ˆ è¿›è¡Œç»¼åˆåˆ†æ...")
            result = analyzer.generate_comprehensive_report(df)
        elif args.type == 'trends':
            print("ğŸ“ˆ è¿›è¡Œè¶‹åŠ¿åˆ†æ...")
            result = {
                'trends': analyzer.analyze_trends(df),
                'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'data_file': args.file
            }
        elif args.type == 'ai':
            print("ğŸ¤– è¿›è¡ŒAIæ·±åº¦åˆ†æ...")
            result = analyzer.analyze_with_ai(df)
        else:
            print(f"âŒ ä¸æ”¯æŒçš„åˆ†æç±»å‹: {args.type}")
            return
        
        # ä¿å­˜åˆ†æç»“æœ
        if args.output:
            analysis_file = analyzer.save_analysis(result, args.output)
        else:
            analysis_file = analyzer.save_analysis(result)
        
        print(f"ğŸ“„ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {analysis_file}")
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        print("\nğŸ“Š è¯¦ç»†åˆ†æç»“æœ:")
        print("=" * 50)
        
        if args.type == 'comprehensive':
            # æ ¸å¿ƒæŒ‡æ ‡
            summary = result.get('summary', {})
            print(f"ğŸ“ˆ æ ¸å¿ƒæŒ‡æ ‡:")
            print(f"   æ€»ç¬”è®°æ•°: {summary.get('total_notes', 0)}")
            print(f"   åˆ†ææ—¶é—´: {summary.get('analysis_time', 'æœªçŸ¥')}")
            
            stats = result.get('statistics', {})
            engagement = stats.get('engagement_stats', {})
            if engagement and not engagement.get('error'):
                print(f"   å¹³å‡ç‚¹èµ: {engagement.get('avg_likes', 0):.1f}")
                print(f"   æœ€é«˜ç‚¹èµ: {engagement.get('max_likes', 0):.1f}")
                print(f"   æœ€ä½ç‚¹èµ: {engagement.get('min_likes', 0):.1f}")
                print(f"   æ€»ç‚¹èµæ•°: {engagement.get('total_likes', 0):.1f}")
            
            basic_stats = stats.get('basic_stats', {})
            print(f"   ä½œè€…æ•°é‡: {basic_stats.get('unique_authors', 0)}")
            print(f"   æ—¶é—´èŒƒå›´: {basic_stats.get('date_range', 'æœªçŸ¥')}")
            
            # çƒ­é—¨ä½œè€…
            trends = result.get('trends', {})
            top_authors = trends.get('top_authors', [])
            if top_authors:
                print(f"\nğŸ‘¥ çƒ­é—¨ä½œè€… TOP5:")
                for i, author in enumerate(top_authors[:5], 1):
                    print(f"   {i}. {author['author']} ({author['count']} ç¯‡)")
            
            # çƒ­é—¨è¯é¢˜
            popular_topics = trends.get('popular_topics', [])
            if popular_topics:
                print(f"\nğŸ”¥ çƒ­é—¨è¯é¢˜ TOP10:")
                for i, topic in enumerate(popular_topics[:10], 1):
                    print(f"   {i}. {topic['keyword']} ({topic['frequency']} æ¬¡)")
            
            # å»ºè®®
            recommendations = result.get('recommendations', {})
            if recommendations:
                print(f"\nğŸ’¡ ç­–ç•¥å»ºè®®:")
                for category, tips in recommendations.items():
                    if tips:
                        print(f"   {category}:")
                        for tip in tips[:3]:  # åªæ˜¾ç¤ºå‰3æ¡
                            print(f"     â€¢ {tip}")
        
        elif args.type == 'trends':
            # è¶‹åŠ¿åˆ†æç»“æœ
            trends = result.get('trends', {})
            print(f"ğŸ“ˆ æ ¸å¿ƒæŒ‡æ ‡:")
            print(f"   æ€»ç¬”è®°æ•°: {trends.get('total_notes', 0)}")
            
            engagement = trends.get('engagement_analysis', {})
            if engagement and not engagement.get('error'):
                print(f"   å¹³å‡ç‚¹èµ: {engagement.get('avg_likes', 0):.1f}")
                print(f"   æœ€é«˜ç‚¹èµ: {engagement.get('max_likes', 0):.1f}")
                print(f"   æœ€ä½ç‚¹èµ: {engagement.get('min_likes', 0):.1f}")
            
            # çƒ­é—¨ä½œè€…
            top_authors = trends.get('top_authors', [])
            if top_authors:
                print(f"\nğŸ‘¥ çƒ­é—¨ä½œè€… TOP5:")
                for i, author in enumerate(top_authors[:5], 1):
                    print(f"   {i}. {author['author']} ({author['count']} ç¯‡)")
            
            # çƒ­é—¨è¯é¢˜
            popular_topics = trends.get('popular_topics', [])
            if popular_topics:
                print(f"\nğŸ”¥ çƒ­é—¨è¯é¢˜ TOP10:")
                for i, topic in enumerate(popular_topics[:10], 1):
                    print(f"   {i}. {topic['keyword']} ({topic['frequency']} æ¬¡)")
            
            # å»ºè®®
            recommendations = trends.get('recommendations', [])
            if recommendations:
                print(f"\nğŸ’¡ ç­–ç•¥å»ºè®®:")
                for rec in recommendations:
                    print(f"   â€¢ {rec}")
        
        elif args.type == 'ai':
            # AIåˆ†æç»“æœ
            ai_result = result.get('ai_analysis', {})
            if ai_result:
                print(f"\nğŸ¤– AIåˆ†ææ‘˜è¦:")
                ai_text = ai_result
                if isinstance(ai_text, str):
                    # åªæ˜¾ç¤ºå‰500ä¸ªå­—ç¬¦
                    if len(ai_text) > 500:
                        ai_text = ai_text[:500] + "..."
                    print(f"   {ai_text}")
            
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

def web_mode(args):
    """Webæ¨¡å¼"""
    print("ğŸŒ å¯åŠ¨Webåº”ç”¨...")
    print(f"   è®¿é—®åœ°å€: http://{config.FLASK_HOST}:{config.FLASK_PORT}")
    print(f"   æ•°æ®ç›®å½•: {config.DATA_DIR}")
    print(f"   è°ƒè¯•æ¨¡å¼: {'å¼€å¯' if config.FLASK_DEBUG else 'å…³é—­'}")
    print(f"   æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    print("=" * 50)
    
    try:
        app.run(
            host=config.FLASK_HOST,
            port=config.FLASK_PORT,
            debug=config.FLASK_DEBUG
        )
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœåŠ¡å·²åœæ­¢")
    except Exception as e:
        print(f"âŒ WebæœåŠ¡å¯åŠ¨å¤±è´¥: {e}")

def list_files_mode(args):
    """æ–‡ä»¶åˆ—è¡¨æ¨¡å¼"""
    print("ğŸ“ æ–‡ä»¶åˆ—è¡¨æ¨¡å¼...")
    
    try:
        data_dir = config.DATA_DIR
        if not os.path.exists(data_dir):
            print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            return
        
        print(f"ğŸ“‚ æ•°æ®ç›®å½•: {data_dir}")
        print("=" * 50)
        
        # åˆ—å‡ºCSVæ–‡ä»¶
        csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]
        if csv_files:
            print("ğŸ“Š æ•°æ®æ–‡ä»¶:")
            for file in sorted(csv_files):
                file_path = os.path.join(data_dir, file)
                size = os.path.getsize(file_path)
                modified = datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%Y-%m-%d %H:%M:%S')
                print(f"   ğŸ“„ {file} ({size} bytes, {modified})")
        else:
            print("   ğŸ“„ æš‚æ— æ•°æ®æ–‡ä»¶")
        
        # åˆ—å‡ºJSONæ–‡ä»¶
        json_files = [f for f in os.listdir(data_dir) if f.endswith('.json')]
        if json_files:
            print("\nğŸ“‹ åˆ†ææ–‡ä»¶:")
            for file in sorted(json_files):
                file_path = os.path.join(data_dir, file)
                size = os.path.getsize(file_path)
                modified = datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%Y-%m-%d %H:%M:%S')
                print(f"   ğŸ“„ {file} ({size} bytes, {modified})")
        else:
            print("   ğŸ“„ æš‚æ— åˆ†ææ–‡ä»¶")
            
    except Exception as e:
        print(f"âŒ è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºé…ç½®å®ä¾‹
    config = Config()
    
    parser = argparse.ArgumentParser(
        description='å°çº¢ä¹¦çƒ­é—¨åšå®¢åˆ†æç³»ç»Ÿ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python main.py crawl -t "ç¾é£Ÿ" -l 20                    # çˆ¬å–ç¾é£Ÿä¸»é¢˜20æ¡ç¬”è®°
  python main.py crawl -t "æ—…è¡Œ" -l 50 -a                 # çˆ¬å–å¹¶åˆ†ææ—…è¡Œä¸»é¢˜
  python main.py analyze -f data/xhs_ç¾é£Ÿ_20241201.csv    # åˆ†ææŒ‡å®šæ–‡ä»¶
  python main.py analyze -f data/xhs_ç¾é£Ÿ_20241201.csv -t comprehensive  # ç»¼åˆåˆ†æ
  python main.py web                                      # å¯åŠ¨Webåº”ç”¨
  python main.py list                                     # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # çˆ¬å–å‘½ä»¤
    crawl_parser = subparsers.add_parser('crawl', help='çˆ¬å–æ•°æ®')
    crawl_parser.add_argument('-t', '--topic', required=True, help='æœç´¢ä¸»é¢˜')
    crawl_parser.add_argument('-l', '--limit', type=int, default=20, help='è·å–æ•°é‡ (é»˜è®¤: 20)')
    crawl_parser.add_argument('-c', '--cookies', help='cookieså­—ç¬¦ä¸²')
    crawl_parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶å')
    crawl_parser.add_argument('-a', '--analyze', action='store_true', help='çˆ¬å–åè‡ªåŠ¨åˆ†æ')
    
    # åˆ†æå‘½ä»¤
    analyze_parser = subparsers.add_parser('analyze', help='åˆ†ææ•°æ®')
    analyze_parser.add_argument('-f', '--file', required=True, help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    analyze_parser.add_argument('-t', '--type', choices=['comprehensive', 'trends', 'ai'], 
                               default='comprehensive', help='åˆ†æç±»å‹ (é»˜è®¤: comprehensive)')
    analyze_parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶å')
    
    # Webå‘½ä»¤
    web_parser = subparsers.add_parser('web', help='å¯åŠ¨Webåº”ç”¨')
    
    # æ–‡ä»¶åˆ—è¡¨å‘½ä»¤
    list_parser = subparsers.add_parser('list', help='åˆ—å‡ºæ–‡ä»¶')
    
    args = parser.parse_args()
    
    if not args.command:
        print_banner()
        parser.print_help()
        return
    
    # ç¡®ä¿å¿…è¦ç›®å½•å­˜åœ¨
    config.ensure_directories()
    
    # æ ¹æ®å‘½ä»¤æ‰§è¡Œç›¸åº”åŠŸèƒ½
    if args.command == 'crawl':
        crawl_mode(args)
    elif args.command == 'analyze':
        analyze_mode(args)
    elif args.command == 'web':
        web_mode(args)
    elif args.command == 'list':
        list_files_mode(args)

if __name__ == '__main__':
    main() 