#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦çƒ­é—¨åšå®¢åˆ†æç³»ç»Ÿ
ä¸»ç¨‹åºå…¥å£
"""

import os
import sys
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import Config
from crawler.xhs_crawler import XHSCrawler
from ai_analyzer.deepseek_analyzer import DeepSeekAnalyzer
from web_app.app import app

def print_banner():
    """æ‰“å°é¡¹ç›®æ¨ªå¹…"""
    banner = """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                              â•‘
    â•‘    å°çº¢ä¹¦çƒ­é—¨åšå®¢åˆ†æç³»ç»Ÿ v1.0.0                              â•‘
    â•‘                                                              â•‘
    â•‘    ğŸ•·ï¸  æ™ºèƒ½çˆ¬å–  |  ğŸ¤– AIåˆ†æ  |  ğŸ“Š å¯è§†åŒ–å±•ç¤º              â•‘
    â•‘                                                              â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)

def crawl_mode(args):
    """çˆ¬å–æ¨¡å¼"""
    print("ğŸ•·ï¸ å¯åŠ¨çˆ¬å–æ¨¡å¼...")
    
    crawler = XHSCrawler()
    result = crawler.crawl_hot_notes(args.topic, args.limit)
    
    if result:
        print(f"âœ… çˆ¬å–å®Œæˆï¼æ•°æ®å·²ä¿å­˜åˆ°: {result}")
        
        if args.analyze:
            print("ğŸ¤– å¼€å§‹AIåˆ†æ...")
            analyzer = DeepSeekAnalyzer()
            
            # åŠ è½½æ•°æ®
            df = analyzer.load_data(result)
            if not df.empty:
                # è¿›è¡Œè¶‹åŠ¿åˆ†æ
                trends = analyzer.analyze_trends(df)
                print("ğŸ“Š è¶‹åŠ¿åˆ†æå®Œæˆ")
                
                # è¿›è¡ŒAIåˆ†æ
                ai_result = analyzer.analyze_with_ai(df)
                print("ğŸ¤– AIåˆ†æå®Œæˆ")
                
                # ä¿å­˜åˆ†æç»“æœ
                analysis_file = analyzer.save_analysis({
                    'trends': trends,
                    'ai_analysis': ai_result,
                    'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'data_file': result
                })
                
                print(f"ğŸ“„ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {analysis_file}")
                
                # æ‰“å°ç®€è¦ç»“æœ
                print("\nğŸ“Š ç®€è¦åˆ†æç»“æœ:")
                print(f"   æ€»ç¬”è®°æ•°: {trends.get('total_notes', 0)}")
                if trends.get('engagement_analysis', {}).get('avg_likes'):
                    print(f"   å¹³å‡ç‚¹èµ: {trends['engagement_analysis']['avg_likes']:.1f}")
                print(f"   çƒ­é—¨ä½œè€…æ•°: {len(trends.get('top_authors', []))}")
                print(f"   çƒ­é—¨è¯é¢˜æ•°: {len(trends.get('popular_topics', []))}")
            else:
                print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
    else:
        print("âŒ çˆ¬å–å¤±è´¥")

def analyze_mode(args):
    """åˆ†ææ¨¡å¼"""
    print("ğŸ¤– å¯åŠ¨åˆ†ææ¨¡å¼...")
    
    if not os.path.exists(args.file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
        return
    
    analyzer = DeepSeekAnalyzer()
    
    # åŠ è½½æ•°æ®
    df = analyzer.load_data(args.file)
    if df.empty:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        return
    
    print(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(df)} æ¡æ•°æ®")
    
    # è¿›è¡Œè¶‹åŠ¿åˆ†æ
    print("ğŸ“ˆ è¿›è¡Œè¶‹åŠ¿åˆ†æ...")
    trends = analyzer.analyze_trends(df)
    
    # è¿›è¡ŒAIåˆ†æ
    print("ğŸ¤– è¿›è¡ŒAIåˆ†æ...")
    ai_result = analyzer.analyze_with_ai(df)
    
    # ä¿å­˜åˆ†æç»“æœ
    analysis_result = {
        'trends': trends,
        'ai_analysis': ai_result,
        'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'data_file': args.file
    }
    
    analysis_file = analyzer.save_analysis(analysis_result)
    print(f"ğŸ“„ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {analysis_file}")
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print("\nğŸ“Š è¯¦ç»†åˆ†æç»“æœ:")
    print("=" * 50)
    
    # æ ¸å¿ƒæŒ‡æ ‡
    print(f"ğŸ“ˆ æ ¸å¿ƒæŒ‡æ ‡:")
    print(f"   æ€»ç¬”è®°æ•°: {trends.get('total_notes', 0)}")
    
    engagement = trends.get('engagement_analysis', {})
    if engagement and not engagement.get('error'):
        print(f"   å¹³å‡ç‚¹èµ: {engagement.get('avg_likes', 0):.1f}")
        print(f"   æœ€é«˜ç‚¹èµ: {engagement.get('max_likes', 0):.1f}")
        print(f"   æœ€ä½ç‚¹èµ: {engagement.get('min_likes', 0):.1f}")
    
    # çƒ­é—¨ä½œè€…
    top_authors = trends.get('top_authors', [])
    if top_authors:
        print(f"\nğŸ‘¥ çƒ­é—¨ä½œè€… TOP5:")
        for i, author in enumerate(top_authors[:5], 1):
            print(f"   {i}. {author['author']} ({author['count']} ç¯‡)")
    
    # çƒ­é—¨è¯é¢˜
    popular_topics = trends.get('popular_topics', [])
    if popular_topics:
        print(f"\nğŸ”¥ çƒ­é—¨è¯é¢˜ TOP10:")
        for i, topic in enumerate(popular_topics[:10], 1):
            print(f"   {i}. {topic['keyword']} ({topic['frequency']} æ¬¡)")
    
    # å»ºè®®
    recommendations = trends.get('recommendations', [])
    if recommendations:
        print(f"\nğŸ’¡ ç­–ç•¥å»ºè®®:")
        for rec in recommendations:
            print(f"   â€¢ {rec}")
    
    # AIåˆ†ææ‘˜è¦
    if ai_result and ai_result.get('ai_analysis'):
        print(f"\nğŸ¤– AIåˆ†ææ‘˜è¦:")
        ai_text = ai_result['ai_analysis']
        # åªæ˜¾ç¤ºå‰500ä¸ªå­—ç¬¦
        if len(ai_text) > 500:
            ai_text = ai_text[:500] + "..."
        print(f"   {ai_text}")

def web_mode(args):
    """Webæ¨¡å¼"""
    print("ğŸŒ å¯åŠ¨Webåº”ç”¨...")
    print(f"   è®¿é—®åœ°å€: http://localhost:{args.port}")
    print(f"   æ•°æ®ç›®å½•: {Config().DATA_DIR}")
    print(f"   æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    
    try:
        app.run(
            host='0.0.0.0',
            port=args.port,
            debug=args.debug
        )
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœåŠ¡å·²åœæ­¢")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å°çº¢ä¹¦çƒ­é—¨åšå®¢åˆ†æç³»ç»Ÿ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python main.py crawl -t "ç¾é£Ÿ" -l 20 -a          # çˆ¬å–ç¾é£Ÿä¸»é¢˜å¹¶åˆ†æ
  python main.py analyze -f data/xhs_ç¾é£Ÿ_20240101.csv  # åˆ†ææŒ‡å®šæ–‡ä»¶
  python main.py web -p 5000                      # å¯åŠ¨WebæœåŠ¡
        """
    )
    
    subparsers = parser.add_subparsers(dest='mode', help='è¿è¡Œæ¨¡å¼')
    
    # çˆ¬å–æ¨¡å¼
    crawl_parser = subparsers.add_parser('crawl', help='çˆ¬å–æ•°æ®')
    crawl_parser.add_argument('-t', '--topic', required=True, help='æœç´¢ä¸»é¢˜')
    crawl_parser.add_argument('-l', '--limit', type=int, default=20, help='è·å–æ•°é‡ (é»˜è®¤: 20)')
    crawl_parser.add_argument('-a', '--analyze', action='store_true', help='çˆ¬å–åç«‹å³åˆ†æ')
    
    # åˆ†ææ¨¡å¼
    analyze_parser = subparsers.add_parser('analyze', help='åˆ†ææ•°æ®')
    analyze_parser.add_argument('-f', '--file', required=True, help='CSVæ–‡ä»¶è·¯å¾„')
    
    # Webæ¨¡å¼
    web_parser = subparsers.add_parser('web', help='å¯åŠ¨Webåº”ç”¨')
    web_parser.add_argument('-p', '--port', type=int, default=5000, help='ç«¯å£å· (é»˜è®¤: 5000)')
    web_parser.add_argument('-d', '--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    if not args.mode:
        parser.print_help()
        return
    
    # æ‰“å°æ¨ªå¹…
    print_banner()
    
    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    config = Config()
    os.makedirs(config.DATA_DIR, exist_ok=True)
    os.makedirs(config.TEMPLATES_DIR, exist_ok=True)
    os.makedirs(config.STATIC_DIR, exist_ok=True)
    
    # æ ¹æ®æ¨¡å¼æ‰§è¡Œç›¸åº”åŠŸèƒ½
    if args.mode == 'crawl':
        crawl_mode(args)
    elif args.mode == 'analyze':
        analyze_mode(args)
    elif args.mode == 'web':
        web_mode(args)

if __name__ == '__main__':
    main() 